# ----------------------------------------------------------------------------
[basic_options] # Basic options for this experiment. Don't change this header
# ----------------------------------------------------------------------------

# random_seed should be a number. Comment out random_seed to use system time as the seed.
# random_seed = 1606103470501

# Search strategy
# Options could include: random, hillclimb, gp, ccegp
# but for now it's only ccegp
strategy = ccegp

# Number of runs per experiment
num_runs_per_experiment = 2

# Number of fitness evals per run (unless strategy overrides it)
num_fitness_evals_per_run = 5000

# Log file path
log_file_path = logs/default.txt

# Attacker Solution file path
attacker_solution_file_path = solutions/defaultAttackerSolution.txt

# Attacker Solution png file path
attacker_solution_png_path = solutions/defaultAttackerSolution.dot

# Defender Solution file path
defender_solution_file_path = solutions/defaultDefenderSolution.txt

# Defender Solution png file path
defender_solution_png_path = solutions/defaultDefenderSolution.dot

# Highest score world file path
high_score_world_file_path = worlds/defaultHighScoreWorld.txt

# If yes, render solutions to defender_solution_png_path with graphviz
render_solutions = yes

# If yes, print the dot files of the attacker and defender at the end of each run
print_dots = no

# If yes (and render_solutions is set), open rendered solutions after the experiment
attacker_open_png = no
defender_open_png = no

# ----------------------------------------------------------------------------
[ccegp_options] # Options for Competitive Co-Evolutionary Genetic Programming Search. Don't change this header
# ----------------------------------------------------------------------------
# Attacker Population size
attacker_mu = 100

# Attacker Offspring size
attacker_lambda = 50

# Attacker Dmax for initialization
attacker_dmax_init = 4

# Attacker Dmax overall (post-initialization)
attacker_dmax_overall = 6

# Attacker Parent selection method.
# Options: fitness_proportional_selection, overselection
# attacker_parent_selection = fitness_proportional_selection
attacker_parent_selection = overselection

# If using Overselection for Attacker, what top % makes the top group?
# 32% = rule of thumb found repeatedly in literature search
attacker_overselection_top = 0.32

# Attacker Mutation probability: chance that variation will be mutation (otherwise recombination)
attacker_p_m = 0.05

# Attacker Survival selection method.
# Options: truncation, k_tournament_without_replacement
attacker_survival_selection = truncation
# attacker_survival_selection = k_tournament_without_replacement

# AttackerTournament size for survival selection, if using k-tournament
attacker_tournament_size_for_survival_selection = 10

# Attacker Parsimony technique
# Options: depth, size
# attacker_parsimony_technique = depth
attacker_parsimony_technique = size

# Attacker Parsimony pressure penalty coefficient
# attacker_pppc = 0.5
# TEMPORARILY set to 0 for debugging without parsimony pressure
attacker_pppc = 0.01

# Defender Population size
defender_mu = 100

# Defender Offspring size
defender_lambda = 50

# Defender Dmax for initialization
defender_dmax_init = 4

# Defender Dmax overall (post-initialization)
defender_dmax_overall = 6

# Defender Parent selection method.
# Options: fitness_proportional_selection, overselection
# defender_parent_selection = fitness_proportional_selection
defender_parent_selection = overselection

# If using Overselection for Defender, what top % makes the top group?
# 32% = rule of thumb found repeatedly in literature search
defender_overselection_top = 0.32

# Defender Mutation probability: chance that variation will be mutation (otherwise recombination)
defender_p_m = 0.05

# Defender Survival selection method.
# Options: truncation, k_tournament_without_replacement
defender_survival_selection = truncation
# defender_survival_selection = k_tournament_without_replacement

# Defender Tournament size for survival selection, if using k-tournament
defender_tournament_size_for_survival_selection = 10

# Defender Parsimony technique
# Options: depth, size
# defender_parsimony_technique = depth
defender_parsimony_technique = size

# Defender Parsimony pressure penalty coefficient
# defender_pppc = 0.5
# TEMPORARILY set to 0 for debugging without parsimony pressure
defender_pppc = 0.01

# Termination method
# If choosing number_of_evals, enter that number in "num_fitness_evals_per_run" above
# Options: number_of_evals, convergence
termination = number_of_evals
# termination = convergence

# n for termination convergence criterion, if using that termination method
n_for_convergence = 100

# Root filename for CIAO data and plot files
ciao_file_path_root = default

# Log file for parsimony pressure data
parsimony_log_file_path = data/defaultParsimonyLog.txt


# ----------------------------------------------------------------------------
[game_options] # Game parameters mostly as defined in the paper.  Don't change this header
# ----------------------------------------------------------------------------

# Does the defender strategy evolve (ccegp) or follow the Saritas model?
# Setting Sartias model allows debugging the attacker evolution more cleanly
defender_strategy = saritas
# defender_strategy = ccegp

# Time (# turns) limit for a game
game_time_limit = 1000

# Which continuous authentication classifiers are we turning on?
ca_classifiers = []
# ca_classifier = ['mouse']

# average user traffic arrivals per time slot ~Poisson(lambda_u)
lambda_u = 1

# user behavior distributed ~N(beta_u, sigma_u)
beta_u = 100
sigma_u = 3

# CA false positive rate in static saritas strategy
eta_u = 0.01

# user reward for successful interaction (currently not used)
nu_r = 0.1

# probability of IDS detecting the attacker listening action
# to be potentially turned into a concave function delta of m, a cost investment
# into the IDS, with delta(0) = 0 and delta(m) = 1 as m -> inf, but that's if we
# want to incorporate the cost investment into the strategy of the defender
delta_l = 0.1

# probability of IDS detecting the attacker attack action
delta_a = 0.2

# probability that the state transitions from blocked in time t to unblocked in
# time t + 1
q = 0.7

# attacker learning rate
gamma = 0.1

# attacker reward discount factor
rho = 0.98

# scalar applied to successful user traffic rewarded to defender
user_bonus = 1

# scalar applied to successful attacks penalized from defender
attacker_penalty = 1

# not yet implemented:
# m := cost of operating IDS (see notes for delta_l)
# C_a := attacker cost of compromising system